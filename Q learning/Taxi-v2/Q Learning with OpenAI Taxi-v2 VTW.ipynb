{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create the environment\n",
    "* Creating the Taxi environment\n",
    "* OpenAI is a library **composed of many environments that can be used to train an agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create the Q-table and initialize it\n",
    "* The Q-matrix consists of a m x n matrix. m represents the n umber of states, and n represents the number of actions per state\n",
    "* OpenAI Gym provides us a way to get these parameters, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size:  6\n",
      "State space:  500\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Action size: \", action_size)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print(\"State space: \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Construct the Q-matrix:\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 50000     # Total episodes to train our algorithm\n",
    "total_test_episodes = 10  # Total episodes to test our algorithm\n",
    "max_steps = 99             # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7        # Learning rate\n",
    "gamma = 0.618              # Discount rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0              # Exploration rate\n",
    "max_epsilon = 1.0          # Exploration probability at start\n",
    "min_epsilon = 0.01         # Minimum exploration probability\n",
    "decay_rate = 0.01          # Exponential decay rate for exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: The Q learn algorithm\n",
    "Now we implement the Q learn algorithm:\n",
    "1. Initialize Q-values ($Q(s,a)$) arbitrarily for all (state, action) pairs.\n",
    "2. While alive or until episode is stopped:...\n",
    "    1. Choos an action ($a$) in the current world state ($s$) based on the current Q-value estimates ($Q(s,\\cdot)$).\n",
    "    2. Take the action ($a$) and observe the outcome state ($s\\prime$) and reward ($r$)\n",
    "    3. Update $Q(s,a) = Q(s,a) + \\alpha\\cdot[r+\\gamma \\cdot \\max_{a\\prime}Q(s\\prime,a\\prime)-Q(s,a)]$ (Bellman equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each episode, run until 'life ends' or 'learning is stopped':\n",
    "for episode in range(total_episodes):\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Choose the action (a) in the current world state (s)\n",
    "        ## First randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)  # Decide between exploit/explore\n",
    "        \n",
    "        ## If number > epsilon, then choose exploitation instead of exploration (taking the biggest reachable Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(q_table[state,:])  # Returns the Q-index for the maximum value in a given state\n",
    "            \n",
    "        ## Else: do a random choice - a.k.a. exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Take the action (a) and observe the outcome state (s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q(s,a) using the Bellman equation\n",
    "        q_table[state, action] = (q_table[state, action] \n",
    "                                  + learning_rate * (reward + gamma * \n",
    "                                                      np.max(q_table[new_state, :]) - q_table[state, action]))\n",
    "        \n",
    "        # Update state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done: finish episode\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    episode += 1\n",
    "    \n",
    "    # Reduce epsilon (to ensure less exploration and more explotation over time)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Use our Q-table to play Taxi\n",
    "* After 'total_episodes' episodes, the modifyed Q-table can be used as a brute-forced 'cheat sheet' to play taxi\n",
    "* By running this cell, you can see the agent play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.#0\n",
      "Score:  8\n",
      "Steps:  12 \n",
      "\n",
      "Ep.#1\n",
      "Score:  5\n",
      "Steps:  15 \n",
      "\n",
      "Ep.#2\n",
      "Score:  7\n",
      "Steps:  13 \n",
      "\n",
      "Ep.#3\n",
      "Score:  5\n",
      "Steps:  15 \n",
      "\n",
      "Ep.#4\n",
      "Score:  9\n",
      "Steps:  11 \n",
      "\n",
      "Ep.#5\n",
      "Score:  3\n",
      "Steps:  17 \n",
      "\n",
      "Ep.#6\n",
      "Score:  10\n",
      "Steps:  10 \n",
      "\n",
      "Ep.#7\n",
      "Score:  7\n",
      "Steps:  13 \n",
      "\n",
      "Ep.#8\n",
      "Score:  11\n",
      "Steps:  9 \n",
      "\n",
      "Ep.#9\n",
      "Score:  6\n",
      "Steps:  14 \n",
      "\n",
      "Score over time: 7.1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "ep_frames = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    ep_frames.append([])\n",
    "    \n",
    "    #print('**************************************************')\n",
    "    #print('EPISODE {}/{}'.format(episode, total_test_episodes))\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Uncomment if you want to see the agent's gameplay\n",
    "        #env.render()\n",
    "        \n",
    "        # Take the action (index) that has the maximum expected future reward, given a state\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        ep_frames[-1].append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'episode': episode\n",
    "        })\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            print('Ep.#{}'.format(episode))\n",
    "            print('Score: ', total_rewards)\n",
    "            print('Steps: ', step, '\\n')\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "                \n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Episode: 9\n",
      "Timestep: 15\n",
      "State: 479\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Acc.Reward: 6\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(ep_frames):\n",
    "    for ep_i, frames in enumerate(ep_frames):\n",
    "        acc_reward = 0\n",
    "        for i, frame in enumerate(frames):\n",
    "            clear_output(wait=True)\n",
    "            print(frame['frame'].getvalue())\n",
    "            print(f\"Episode: {frame['episode']}\")\n",
    "            print(f\"Timestep: {i + 1}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            acc_reward += frame['reward']\n",
    "            print(f\"Acc.Reward: {acc_reward}\")\n",
    "            tanh = lambda x, x0=-1, xN=1, y0=-1, yN=1: (yN-y0)/2 * np.tanh(2*np.pi*((x-x0)/(xN-x0)) - np.pi) + y0 + (yN-y0)/2  # tanh that goes from 0 to 1\n",
    "            sleep(tanh(ep_i, 0, len(ep_frames), 1, 0.1))\n",
    "        \n",
    "print_frames(ep_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "func = lambda x, x0=-1, xN=1, y0=-1, yN=1: (yN-y0)/2 * np.tanh(2*np.pi*((x-x0)/(xN-x0)) - np.pi) + y0 + (yN-y0)/2  # tanh that goes from 0 to 1\n",
    "xpts = np.linspace(0, len(ep_frames), 100)\n",
    "\n",
    "plt.plot(xpts, func(xpts, 0, len(ep_frames), 1, 0.1))\n",
    "plt.axhline(y=0.1, color='r', linestyle='--')\n",
    "\n",
    "plt.title(\"Scaled tanh()-function\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Seconds per frame [1/fps]\")\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ep_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
